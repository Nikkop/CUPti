{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"Models/GoogleNews-vectors-negative300.bin\", binary=True) \n",
    "\n",
    "vocab = np.array(model.index_to_key)\n",
    "word_vector=[]\n",
    "for w in vocab:\n",
    "    #vector=model[w] / np.sum(model[w])\n",
    "    word_vector.append(model[w])\n",
    "\n",
    "del model\n",
    "w2v_matrix=np.asarray(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get list of comments\n",
    "df = pd.read_csv('data/arr_data.csv')\n",
    "comments = list(df['lem_comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Creation of reduced w2v_matrix model with 1-gram through 6-gram\n",
    "# discard nan items\n",
    "all_txt = [i for i in comments if type(i) == str]\n",
    "all_txt = \" \".join(all_txt)\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation)   \n",
    "all_words = all_txt.translate(translate_table).split()\n",
    "\n",
    "ngram_start = 1\n",
    "ngram_end = 6\n",
    "all_words_ngram = [\"_\".join(all_words[i:i + ngram]) for ngram in range(ngram_start, ngram_end +1) for i in range(len(all_words) - (ngram - 1))]\n",
    "unique_words_list = list(set(all_words_ngram))\n",
    "\n",
    "# Build hash map from word to index\n",
    "vocab_map = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "# Get indices of items in vocab that are contained in unique_words_list\n",
    "idx_with_Nones = np.array([vocab_map.get(word) for word in unique_words_list])\n",
    "idx = [i for i in idx_with_Nones if i is not None]\n",
    "\n",
    "# Reduce w2v_matrix size by limiting to all unique words in this course.\n",
    "vocab = vocab[idx]\n",
    "w2v_matrix = w2v_matrix[idx]\n",
    "\n",
    "print(\"Completed reduction. Dumping to pickle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( w2v_matrix, open( \"data/w2v_matrix_for_HarvardX__HDS_3221_2X__1T2016.p\", \"wb\" ) )\n",
    "pickle.dump( vocab, open( \"/data/vocab_for_HarvardX__HDS_3221_2X__1T2016.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
